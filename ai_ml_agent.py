import streamlit as st
import requests
import json
import pandas as pd
import zipfile
import io
import os
import numpy as np
from urllib.parse import quote
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report
from sklearn.metrics import roc_curve, auc, confusion_matrix, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

st.set_page_config(page_title="AI ML Agent", layout="wide")

# Initialize session state for conversation
if 'conversation' not in st.session_state:
    st.session_state.conversation = []
if 'user_preferences' not in st.session_state:
    st.session_state.user_preferences = {}
if 'current_dataset' not in st.session_state:
    st.session_state.current_dataset = None
if 'ml_results' not in st.session_state:
    st.session_state.ml_results = {}

# AI Agent Functions
def get_ai_response(user_message, context=""):
    """Enhanced AI agent responses with comprehensive understanding"""
    
    user_lower = user_message.lower()
    
    # Comprehensive response system
    if any(word in user_lower for word in ["hello", "hi", "hey", "start", "begin"]):
        return "ğŸ‘‹ Hello! I'm your AI ML Assistant. I can help you with:\n\n" \
               "â€¢ **Dataset Selection** - Find the perfect dataset for your project\n" \
               "â€¢ **ML Pipeline Guidance** - Choose what analysis you need\n" \
               "â€¢ **Output Explanations** - Understand every chart and result\n" \
               "â€¢ **Step-by-step Guidance** - Walk through the entire ML process\n" \
               "â€¢ **Answer Any Questions** - Ask me anything about ML, data science, or your results\n\n" \
               "What would you like to work on today?"
    
    elif any(word in user_lower for word in ["correlation", "correlation matrix", "heatmap"]):
        return explain_correlation_matrix_detailed()
    
    elif any(word in user_lower for word in ["accuracy", "precision", "recall", "f1", "classification"]):
        return explain_classification_metrics()
    
    elif any(word in user_lower for word in ["mse", "rmse", "r2", "regression", "prediction"]):
        return explain_regression_metrics()
    
    elif any(word in user_lower for word in ["histogram", "distribution", "plot", "chart"]):
        return explain_visualizations()
    
    elif any(word in user_lower for word in ["missing", "null", "na", "empty"]):
        return explain_missing_data()
    
    elif any(word in user_lower for word in ["outlier", "anomaly", "extreme"]):
        return explain_outliers()
    
    elif any(word in user_lower for word in ["feature", "variable", "column"]):
        return explain_features()
    
    elif any(word in user_lower for word in ["model", "algorithm", "training"]):
        return explain_ml_models()
    
    elif any(word in user_lower for word in ["overfitting", "underfitting", "bias", "variance"]):
        return explain_model_performance()
    
    elif any(word in user_lower for word in ["cross validation", "validation", "test"]):
        return explain_validation()
    
    elif any(word in user_lower for word in ["preprocessing", "scaling", "encoding"]):
        return explain_preprocessing()
    
    elif any(word in user_lower for word in ["dataset", "data", "csv", "file"]):
        return "ğŸ“Š **Dataset Selection Help:**\n\n" \
               "Tell me about your project:\n" \
               "â€¢ What type of data are you interested in? (e.g., sales, customer, medical, financial)\n" \
               "â€¢ What's your goal? (prediction, classification, analysis)\n" \
               "â€¢ Any specific domain? (business, healthcare, sports, etc.)\n\n" \
               "I'll recommend the best datasets for you!"
    
    elif any(word in user_lower for word in ["pipeline", "analysis", "workflow"]):
        return "ğŸ”§ **ML Pipeline Options:**\n\n" \
               "Choose what you need:\n" \
               "â€¢ **EDA Only** - Data exploration and visualization\n" \
               "â€¢ **Preprocessing** - Data cleaning and preparation\n" \
               "â€¢ **Model Training** - Build and train ML models\n" \
               "â€¢ **Full Pipeline** - Complete end-to-end analysis\n" \
               "â€¢ **Custom** - Tell me exactly what you want\n\n" \
               "What level of analysis do you need?"
    
    elif any(word in user_lower for word in ["explain", "what", "how", "why", "meaning", "understand"]):
        return "ğŸ“š **I'll Explain Everything:**\n\n" \
               "For each output, I'll provide:\n" \
               "â€¢ **What it shows** - Clear description\n" \
               "â€¢ **Why it matters** - Business/technical importance\n" \
               "â€¢ **How to interpret** - Reading the results\n" \
               "â€¢ **Next steps** - What to do with the insights\n\n" \
               "Just ask me about any chart or result you see!"
    
    elif any(word in user_lower for word in ["help", "stuck", "confused", "don't understand"]):
        return "ğŸ†˜ **I'm here to help!**\n\n" \
               "You can ask me about:\n" \
               "â€¢ Any chart, graph, or visualization you see\n" \
               "â€¢ Model results and metrics\n" \
               "â€¢ Data preprocessing steps\n" \
               "â€¢ Next steps in your analysis\n" \
               "â€¢ General ML concepts\n\n" \
               "What specific part would you like me to explain?"
    
    else:
        # Intelligent response based on context
        return f"ğŸ¤– I understand you're asking about: '{user_message}'\n\n" \
               f"Let me help you with that! I can explain:\n" \
               f"â€¢ **Data Analysis Results** - Any charts, graphs, or statistics\n" \
               f"â€¢ **ML Model Outputs** - Accuracy, predictions, performance metrics\n" \
               f"â€¢ **Data Preprocessing** - Cleaning, scaling, encoding steps\n" \
               f"â€¢ **Next Steps** - What to do with your results\n\n" \
               f"Could you be more specific about what you'd like me to explain?"

# Comprehensive Explanation Functions
def explain_correlation_matrix_detailed():
    """Detailed explanation of correlation matrix"""
    return "ğŸ“Š **Correlation Matrix - Complete Guide:**\n\n" \
           "**What it shows:**\n" \
           "â€¢ A heatmap showing relationships between all numeric variables\n" \
           "â€¢ Each cell shows correlation coefficient (-1 to +1)\n" \
           "â€¢ Colors: Red = positive, Blue = negative, White = no correlation\n\n" \
           "**How to read it:**\n" \
           "â€¢ **1.0**: Perfect positive correlation (variables move together)\n" \
           "â€¢ **0.7-0.9**: Strong positive correlation\n" \
           "â€¢ **0.3-0.7**: Moderate positive correlation\n" \
           "â€¢ **0.0-0.3**: Weak positive correlation\n" \
           "â€¢ **0.0**: No linear relationship\n" \
           "â€¢ **-0.3 to 0.0**: Weak negative correlation\n" \
           "â€¢ **-0.7 to -0.3**: Moderate negative correlation\n" \
           "â€¢ **-1.0**: Perfect negative correlation (variables move opposite)\n\n" \
           "**Why it matters:**\n" \
           "â€¢ **Feature Selection**: Remove highly correlated features to avoid redundancy\n" \
           "â€¢ **Multicollinearity**: High correlations can cause model instability\n" \
           "â€¢ **Business Insights**: Understand which factors influence each other\n" \
           "â€¢ **Model Building**: Choose features that are predictive but not redundant\n\n" \
           "**Red flags to watch:**\n" \
           "â€¢ Correlations > 0.8: Consider removing one variable\n" \
           "â€¢ Perfect correlations (1.0 or -1.0): Variables are identical\n" \
           "â€¢ Many high correlations: Data might be over-engineered"

def explain_correlation_matrix(df=None):
    """Explain correlation matrix results"""
    if df is not None:
        explanation = "ğŸ“Š **Correlation Matrix Explanation:**\n\n"
        # Calculate correlation matrix
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            
            # Find strongest correlations
            corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.5:  # Strong correlation threshold
                        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
            
            explanation += "**What it shows:**\n"
            explanation += "â€¢ Colors represent correlation strength (-1 to +1)\n"
            explanation += "â€¢ Red = Strong positive correlation\n"
            explanation += "â€¢ Blue = Strong negative correlation\n"
            explanation += "â€¢ White = No correlation\n\n"
            
            if corr_pairs:
                explanation += "**Strong Correlations Found:**\n"
                for col1, col2, corr in corr_pairs[:3]:  # Show top 3
                    explanation += f"â€¢ {col1} â†” {col2}: {corr:.2f}\n"
                explanation += "\n"
            
            explanation += "**Why it matters:**\n"
            explanation += "â€¢ Helps identify relationships between variables\n"
            explanation += "â€¢ Useful for feature selection\n"
            explanation += "â€¢ Reveals potential multicollinearity\n"
            explanation += "â€¢ Guides model building decisions\n\n"
            
            explanation += "**How to interpret:**\n"
            explanation += "â€¢ Values close to 1: Strong positive relationship\n"
            explanation += "â€¢ Values close to -1: Strong negative relationship\n"
            explanation += "â€¢ Values close to 0: No linear relationship\n"
        else:
            explanation += "No numeric columns found for correlation analysis."
        
        return explanation
    else:
        return explain_correlation_matrix_detailed()

def explain_classification_metrics():
    """Explain classification model metrics"""
    return "ğŸ¯ **Classification Metrics - Complete Guide:**\n\n" \
           "**Accuracy:**\n" \
           "â€¢ **What**: Percentage of correct predictions\n" \
           "â€¢ **Formula**: (True Positives + True Negatives) / Total Predictions\n" \
           "â€¢ **Good**: > 80% for most problems\n" \
           "â€¢ **Warning**: Can be misleading with imbalanced data\n\n" \
           "**Precision:**\n" \
           "â€¢ **What**: Of all positive predictions, how many were actually positive?\n" \
           "â€¢ **Formula**: True Positives / (True Positives + False Positives)\n" \
           "â€¢ **Use**: When false positives are costly (e.g., spam detection)\n" \
           "â€¢ **Good**: > 0.8 for most applications\n\n" \
           "**Recall (Sensitivity):**\n" \
           "â€¢ **What**: Of all actual positives, how many did we catch?\n" \
           "â€¢ **Formula**: True Positives / (True Positives + False Negatives)\n" \
           "â€¢ **Use**: When false negatives are costly (e.g., disease detection)\n" \
           "â€¢ **Good**: > 0.8 for most applications\n\n" \
           "**F1-Score:**\n" \
           "â€¢ **What**: Balance between precision and recall\n" \
           "â€¢ **Formula**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n" \
           "â€¢ **Use**: When you need balanced performance\n" \
           "â€¢ **Good**: > 0.7 for most applications\n\n" \
           "**Confusion Matrix:**\n" \
           "â€¢ **True Positives**: Correctly predicted positive cases\n" \
           "â€¢ **True Negatives**: Correctly predicted negative cases\n" \
           "â€¢ **False Positives**: Incorrectly predicted as positive\n" \
           "â€¢ **False Negatives**: Incorrectly predicted as negative"

def explain_regression_metrics():
    """Explain regression model metrics"""
    return "ğŸ“ˆ **Regression Metrics - Complete Guide:**\n\n" \
           "**Mean Squared Error (MSE):**\n" \
           "â€¢ **What**: Average of squared differences between predicted and actual values\n" \
           "â€¢ **Formula**: Î£(Actual - Predicted)Â² / n\n" \
           "â€¢ **Units**: Same as target variable, but squared\n" \
           "â€¢ **Lower is better**: 0 = perfect predictions\n" \
           "â€¢ **Sensitive to outliers**: Large errors have big impact\n\n" \
           "**Root Mean Squared Error (RMSE):**\n" \
           "â€¢ **What**: Square root of MSE\n" \
           "â€¢ **Formula**: âˆšMSE\n" \
           "â€¢ **Units**: Same as target variable\n" \
           "â€¢ **Lower is better**: 0 = perfect predictions\n" \
           "â€¢ **Interpretation**: Average prediction error in original units\n\n" \
           "**Mean Absolute Error (MAE):**\n" \
           "â€¢ **What**: Average absolute difference between predicted and actual values\n" \
           "â€¢ **Formula**: Î£|Actual - Predicted| / n\n" \
           "â€¢ **Units**: Same as target variable\n" \
           "â€¢ **Lower is better**: 0 = perfect predictions\n" \
           "â€¢ **Less sensitive to outliers**: More robust than MSE\n\n" \
           "**RÂ² Score (Coefficient of Determination):**\n" \
           "â€¢ **What**: Proportion of variance in target variable explained by the model\n" \
           "â€¢ **Formula**: 1 - (SS_res / SS_tot)\n" \
           "â€¢ **Range**: 0 to 1 (can be negative for very bad models)\n" \
           "â€¢ **Interpretation**:\n" \
           "  - 1.0: Perfect predictions\n" \
           "  - 0.8-1.0: Excellent model\n" \
           "  - 0.6-0.8: Good model\n" \
           "  - 0.4-0.6: Moderate model\n" \
           "  - 0.0-0.4: Poor model\n" \
           "  - < 0.0: Worse than just using the mean"

def explain_visualizations():
    """Explain different types of visualizations"""
    return "ğŸ“Š **Data Visualizations - Complete Guide:**\n\n" \
           "**Histograms:**\n" \
           "â€¢ **What**: Shows distribution of a single variable\n" \
           "â€¢ **Use**: Understand data shape, find patterns, detect outliers\n" \
           "â€¢ **Read**: Height = frequency, Width = value range\n" \
           "â€¢ **Patterns**: Normal (bell curve), Skewed, Bimodal, Uniform\n\n" \
           "**Scatter Plots:**\n" \
           "â€¢ **What**: Shows relationship between two variables\n" \
           "â€¢ **Use**: Find correlations, patterns, clusters\n" \
           "â€¢ **Read**: X-axis = variable 1, Y-axis = variable 2\n" \
           "â€¢ **Patterns**: Linear, Curved, Clustered, Random\n\n" \
           "**Box Plots:**\n" \
           "â€¢ **What**: Shows distribution summary (quartiles, outliers)\n" \
           "â€¢ **Use**: Compare groups, detect outliers, understand spread\n" \
           "â€¢ **Read**: Box = middle 50%, Line = median, Whiskers = range\n" \
           "â€¢ **Outliers**: Points beyond whiskers\n\n" \
           "**Bar Charts:**\n" \
           "â€¢ **What**: Compares categories or groups\n" \
           "â€¢ **Use**: Show counts, frequencies, comparisons\n" \
           "â€¢ **Read**: Height = value, Categories on X-axis\n" \
           "â€¢ **Types**: Vertical, Horizontal, Stacked, Grouped\n\n" \
           "**Line Charts:**\n" \
           "â€¢ **What**: Shows trends over time or sequence\n" \
           "â€¢ **Use**: Track changes, identify patterns, forecast\n" \
           "â€¢ **Read**: X-axis = time/sequence, Y-axis = values\n" \
           "â€¢ **Patterns**: Increasing, Decreasing, Seasonal, Cyclical"

def explain_missing_data():
    """Explain missing data analysis"""
    return "â“ **Missing Data - Complete Guide:**\n\n" \
           "**What is Missing Data?**\n" \
           "â€¢ Values that are not recorded or available\n" \
           "â€¢ Shown as NaN, None, empty cells, or special codes\n" \
           "â€¢ Can occur due to data collection issues, privacy, or errors\n\n" \
           "**Types of Missing Data:**\n" \
           "â€¢ **MCAR (Missing Completely at Random)**: No pattern, random\n" \
           "â€¢ **MAR (Missing at Random)**: Pattern exists but not in missing values\n" \
           "â€¢ **MNAR (Missing Not at Random)**: Pattern in missing values themselves\n\n" \
           "**Impact on Analysis:**\n" \
           "â€¢ **Reduces sample size**: Fewer observations for analysis\n" \
           "â€¢ **Biases results**: If missing data has patterns\n" \
           "â€¢ **Breaks algorithms**: Many ML models can't handle missing values\n" \
           "â€¢ **Reduces power**: Statistical tests become less reliable\n\n" \
           "**Handling Strategies:**\n" \
           "â€¢ **Deletion**: Remove rows/columns with missing data\n" \
           "â€¢ **Imputation**: Fill missing values with estimates\n" \
           "  - Mean/Median: For numeric data\n" \
           "  - Mode: For categorical data\n" \
           "  - Forward/Backward fill: For time series\n" \
           "  - Advanced: KNN, regression-based imputation\n" \
           "â€¢ **Flagging**: Create indicator variables for missing data\n\n" \
           "**When to worry:**\n" \
           "â€¢ > 5% missing: Investigate patterns\n" \
           "â€¢ > 20% missing: Consider if data is usable\n" \
           "â€¢ > 50% missing: May need to exclude variable"

def explain_outliers():
    """Explain outlier detection and handling"""
    return "ğŸš¨ **Outliers - Complete Guide:**\n\n" \
           "**What are Outliers?**\n" \
           "â€¢ Data points that are significantly different from others\n" \
           "â€¢ Values that fall outside the normal range\n" \
           "â€¢ Can be detected using statistical methods or visual inspection\n\n" \
           "**Types of Outliers:**\n" \
           "â€¢ **Point Outliers**: Individual data points\n" \
           "â€¢ **Contextual Outliers**: Unusual in specific context\n" \
           "â€¢ **Collective Outliers**: Group of points that are unusual together\n\n" \
           "**Detection Methods:**\n" \
           "â€¢ **IQR Method**: Q3 + 1.5Ã—IQR or Q1 - 1.5Ã—IQR\n" \
           "â€¢ **Z-Score**: Values with |z| > 3\n" \
           "â€¢ **Modified Z-Score**: Using median absolute deviation\n" \
           "â€¢ **Visual**: Box plots, scatter plots, histograms\n\n" \
           "**Causes of Outliers:**\n" \
           "â€¢ **Data Entry Errors**: Typos, wrong units\n" \
           "â€¢ **Measurement Errors**: Equipment malfunction\n" \
           "â€¢ **Natural Variation**: Extreme but valid values\n" \
           "â€¢ **Fraud/Anomalies**: Suspicious activities\n\n" \
           "**Impact on Analysis:**\n" \
           "â€¢ **Skew Statistics**: Mean, standard deviation\n" \
           "â€¢ **Affect Models**: Regression, clustering algorithms\n" \
           "â€¢ **Reduce Accuracy**: Can mislead model training\n" \
           "â€¢ **Mask Patterns**: Hide underlying relationships\n\n" \
           "**Handling Strategies:**\n" \
           "â€¢ **Investigate**: Understand why they exist\n" \
           "â€¢ **Remove**: If clearly errors\n" \
           "â€¢ **Transform**: Log, square root transformations\n" \
           "â€¢ **Cap/Winsorize**: Replace with threshold values\n" \
           "â€¢ **Keep**: If they represent important information"

def explain_features():
    """Explain feature engineering and selection"""
    return "ğŸ”§ **Features - Complete Guide:**\n\n" \
           "**What are Features?**\n" \
           "â€¢ Variables used to predict the target\n" \
           "â€¢ Input data for machine learning models\n" \
           "â€¢ Can be raw data or engineered/transformed variables\n\n" \
           "**Types of Features:**\n" \
           "â€¢ **Numeric**: Continuous values (age, price, temperature)\n" \
           "â€¢ **Categorical**: Discrete categories (color, brand, status)\n" \
           "â€¢ **Binary**: Two categories (yes/no, true/false)\n" \
           "â€¢ **Ordinal**: Ordered categories (rating, grade)\n" \
           "â€¢ **Text**: String data (reviews, descriptions)\n" \
           "â€¢ **Date/Time**: Temporal data (timestamps, dates)\n\n" \
           "**Feature Engineering:**\n" \
           "â€¢ **Creating New Features**:\n" \
           "  - Mathematical operations (sum, difference, ratio)\n" \
           "  - Binning continuous variables\n" \
           "  - Extracting from text (word count, sentiment)\n" \
           "  - Time-based features (day of week, season)\n" \
           "â€¢ **Transforming Features**:\n" \
           "  - Scaling (normalization, standardization)\n" \
           "  - Encoding (one-hot, label encoding)\n" \
           "  - Log transformation for skewed data\n\n" \
           "**Feature Selection:**\n" \
           "â€¢ **Filter Methods**: Statistical tests, correlation\n" \
           "â€¢ **Wrapper Methods**: Forward/backward selection\n" \
           "â€¢ **Embedded Methods**: Lasso, tree-based importance\n" \
           "â€¢ **Domain Knowledge**: Expert judgment\n\n" \
           "**Good Features:**\n" \
           "â€¢ **Relevant**: Related to the target variable\n" \
           "â€¢ **Non-redundant**: Not highly correlated with others\n" \
           "â€¢ **Complete**: Few missing values\n" \
           "â€¢ **Consistent**: Same format and meaning\n" \
           "â€¢ **Scalable**: Easy to obtain for new data"

def explain_ml_models():
    """Explain different ML algorithms"""
    return "ğŸ¤– **Machine Learning Models - Complete Guide:**\n\n" \
           "**Supervised Learning (with target variable):**\n" \
           "â€¢ **Classification**: Predict categories/classes\n" \
           "  - Logistic Regression: Linear decision boundary\n" \
           "  - Random Forest: Ensemble of decision trees\n" \
           "  - SVM: Finds optimal separating hyperplane\n" \
           "  - Naive Bayes: Probabilistic classifier\n" \
           "â€¢ **Regression**: Predict continuous values\n" \
           "  - Linear Regression: Linear relationship\n" \
           "  - Random Forest: Ensemble for regression\n" \
           "  - Polynomial Regression: Non-linear relationships\n\n" \
           "**Unsupervised Learning (no target variable):**\n" \
           "â€¢ **Clustering**: Group similar data points\n" \
           "  - K-Means: Spherical clusters\n" \
           "  - Hierarchical: Tree-like clustering\n" \
           "  - DBSCAN: Density-based clustering\n" \
           "â€¢ **Dimensionality Reduction**: Reduce feature count\n" \
           "  - PCA: Linear dimensionality reduction\n" \
           "  - t-SNE: Non-linear visualization\n\n" \
           "**Model Selection Criteria:**\n" \
           "â€¢ **Data Size**: Small data â†’ simple models\n" \
           "â€¢ **Linearity**: Linear data â†’ linear models\n" \
           "â€¢ **Interpretability**: Need explanations â†’ linear/logistic\n" \
           "â€¢ **Performance**: Need accuracy â†’ ensemble methods\n" \
           "â€¢ **Speed**: Need fast predictions â†’ simple models\n\n" \
           "**Model Performance:**\n" \
           "â€¢ **Training**: How well model fits training data\n" \
           "â€¢ **Validation**: How well model generalizes\n" \
           "â€¢ **Overfitting**: Good on training, poor on new data\n" \
           "â€¢ **Underfitting**: Poor on both training and new data"

def explain_model_performance():
    """Explain model performance concepts"""
    return "ğŸ“Š **Model Performance - Complete Guide:**\n\n" \
           "**Overfitting:**\n" \
           "â€¢ **What**: Model learns training data too well\n" \
           "â€¢ **Signs**: High training accuracy, low validation accuracy\n" \
           "â€¢ **Causes**: Too complex model, too little data, too many features\n" \
           "â€¢ **Solutions**:\n" \
           "  - Simplify model (reduce complexity)\n" \
           "  - More training data\n" \
           "  - Regularization (L1/L2)\n" \
           "  - Cross-validation\n" \
           "  - Early stopping\n\n" \
           "**Underfitting:**\n" \
           "â€¢ **What**: Model is too simple to capture patterns\n" \
           "â€¢ **Signs**: Low training and validation accuracy\n" \
           "â€¢ **Causes**: Too simple model, insufficient features\n" \
           "â€¢ **Solutions**:\n" \
           "  - Increase model complexity\n" \
           "  - Add more features\n" \
           "  - Feature engineering\n" \
           "  - Different algorithm\n\n" \
           "**Bias vs Variance:**\n" \
           "â€¢ **Bias**: Error from oversimplifying assumptions\n" \
           "  - High bias = underfitting\n" \
           "  - Low bias = model can capture complexity\n" \
           "â€¢ **Variance**: Error from sensitivity to training data\n" \
           "  - High variance = overfitting\n" \
           "  - Low variance = model is stable\n" \
           "â€¢ **Bias-Variance Tradeoff**: Balancing both errors\n\n" \
           "**Model Validation:**\n" \
           "â€¢ **Train/Test Split**: Separate data for training and testing\n" \
           "â€¢ **Cross-Validation**: Multiple train/test splits\n" \
           "â€¢ **Holdout Set**: Final test on unseen data\n" \
           "â€¢ **Time Series**: Use temporal splits for time data"

def explain_validation():
    """Explain validation techniques"""
    return "âœ… **Model Validation - Complete Guide:**\n\n" \
           "**Why Validate?**\n" \
           "â€¢ **Prevent Overfitting**: Test on unseen data\n" \
           "â€¢ **Estimate Performance**: How well model will work in practice\n" \
           "â€¢ **Compare Models**: Choose best performing algorithm\n" \
           "â€¢ **Tune Parameters**: Find optimal hyperparameters\n\n" \
           "**Validation Methods:**\n" \
           "â€¢ **Holdout Validation**:\n" \
           "  - Split: 70% train, 15% validation, 15% test\n" \
           "  - Use: Quick validation, large datasets\n" \
           "  - Risk: Results depend on random split\n\n" \
           "â€¢ **K-Fold Cross-Validation**:\n" \
           "  - Split data into k folds (usually 5 or 10)\n" \
           "  - Train on k-1 folds, test on 1 fold\n" \
           "  - Repeat k times, average results\n" \
           "  - Use: More reliable estimates\n\n" \
           "â€¢ **Stratified K-Fold**:\n" \
           "  - Maintains class distribution in each fold\n" \
           "  - Use: Imbalanced datasets\n\n" \
           "â€¢ **Time Series Validation**:\n" \
           "  - Use past data to predict future\n" \
           "  - Expanding or rolling window\n" \
           "  - Use: Temporal data\n\n" \
           "**Validation Metrics:**\n" \
           "â€¢ **Classification**: Accuracy, Precision, Recall, F1\n" \
           "â€¢ **Regression**: MSE, RMSE, MAE, RÂ²\n" \
           "â€¢ **Business Metrics**: Cost, profit, customer satisfaction\n\n" \
           "**Best Practices:**\n" \
           "â€¢ **Never use test set for model selection**\n" \
           "â€¢ **Use validation set for hyperparameter tuning**\n" \
           "â€¢ **Report final results on test set only**\n" \
           "â€¢ **Use multiple random seeds for stability**"

def explain_preprocessing():
    """Explain data preprocessing steps"""
    return "ğŸ”§ **Data Preprocessing - Complete Guide:**\n\n" \
           "**Why Preprocess?**\n" \
           "â€¢ **Clean Data**: Remove errors and inconsistencies\n" \
           "â€¢ **Format Data**: Make it suitable for algorithms\n" \
           "â€¢ **Improve Performance**: Better model accuracy\n" \
           "â€¢ **Handle Missing Values**: Deal with incomplete data\n\n" \
           "**Data Cleaning:**\n" \
           "â€¢ **Remove Duplicates**: Eliminate repeated records\n" \
           "â€¢ **Handle Outliers**: Detect and treat extreme values\n" \
           "â€¢ **Fix Inconsistencies**: Standardize formats\n" \
           "â€¢ **Validate Data**: Check for logical errors\n\n" \
           "**Handling Missing Data:**\n" \
           "â€¢ **Deletion**: Remove rows/columns with missing values\n" \
           "â€¢ **Imputation**: Fill missing values\n" \
           "  - Mean/Median: For numeric data\n" \
           "  - Mode: For categorical data\n" \
           "  - Forward/Backward fill: For time series\n" \
           "  - Advanced methods: KNN, regression imputation\n\n" \
           "**Feature Encoding:**\n" \
           "â€¢ **Label Encoding**: Convert categories to numbers\n" \
           "â€¢ **One-Hot Encoding**: Create binary columns for categories\n" \
           "â€¢ **Target Encoding**: Use target variable for encoding\n" \
           "â€¢ **Ordinal Encoding**: For ordered categories\n\n" \
           "**Feature Scaling:**\n" \
           "â€¢ **Standardization**: (x - mean) / std\n" \
           "  - Mean = 0, Std = 1\n" \
           "  - Use: Normal distribution, distance-based algorithms\n" \
           "â€¢ **Normalization**: (x - min) / (max - min)\n" \
           "  - Range: 0 to 1\n" \
           "  - Use: Neural networks, algorithms sensitive to scale\n" \
           "â€¢ **Robust Scaling**: (x - median) / IQR\n" \
           "  - Use: Data with outliers\n\n" \
           "**Feature Transformation:**\n" \
           "â€¢ **Log Transformation**: Reduce skewness\n" \
           "â€¢ **Square Root**: For count data\n" \
           "â€¢ **Box-Cox**: Optimal transformation\n" \
           "â€¢ **Polynomial**: Create interaction terms"

def explain_model_results(model_type, results):
    """Explain ML model results"""
    explanation = f"ğŸ¤– **{model_type} Model Results Explanation:**\n\n"
    
    if model_type == "Classification":
        if 'accuracy' in results:
            explanation += f"**Accuracy: {results['accuracy']:.2%}**\n"
            explanation += f"â€¢ This means the model correctly predicted {results['accuracy']:.1%} of all cases\n"
            explanation += f"â€¢ Higher is better (perfect = 100%)\n\n"
        
        if 'classification_report' in results:
            explanation += "**Classification Report:**\n"
            explanation += "â€¢ **Precision**: How many predicted positives were actually positive\n"
            explanation += "â€¢ **Recall**: How many actual positives were correctly identified\n"
            explanation += "â€¢ **F1-Score**: Balance between precision and recall\n\n"
    
    elif model_type == "Regression":
        if 'mse' in results:
            explanation += f"**Mean Squared Error: {results['mse']:.2f}**\n"
            explanation += f"â€¢ Average squared difference between predicted and actual values\n"
            explanation += f"â€¢ Lower is better (perfect = 0)\n\n"
        
        if 'r2' in results:
            explanation += f"**RÂ² Score: {results['r2']:.2f}**\n"
            explanation += f"â€¢ Proportion of variance explained by the model\n"
            explanation += f"â€¢ Higher is better (perfect = 1.0)\n\n"
    
    explanation += "**What this means:**\n"
    explanation += "â€¢ The model has been trained and tested\n"
    explanation += "â€¢ These metrics show how well it performs\n"
    explanation += "â€¢ Use these to compare different models\n\n"
    
    explanation += "**Next steps:**\n"
    explanation += "â€¢ Try different algorithms to improve performance\n"
    explanation += "â€¢ Feature engineering might help\n"
    explanation += "â€¢ Consider ensemble methods for better results"
    
    return explanation

def recommend_datasets(user_preferences):
    """Recommend datasets based on user preferences"""
    recommendations = []
    
    # Popular datasets with descriptions
    all_datasets = {
        "House Prices": {
            "description": "Predict house prices using features like size, location, age",
            "type": "Regression",
            "domain": "Real Estate",
            "difficulty": "Intermediate",
            "tags": ["real estate", "regression", "prediction", "housing"]
        },
        "Titanic": {
            "description": "Predict passenger survival on the Titanic",
            "type": "Classification", 
            "domain": "Historical",
            "difficulty": "Beginner",
            "tags": ["classification", "survival", "beginner", "binary"]
        },
        "Wine Quality": {
            "description": "Predict wine quality based on chemical properties",
            "type": "Classification",
            "domain": "Food & Beverage", 
            "difficulty": "Intermediate",
            "tags": ["classification", "wine", "quality", "chemistry"]
        },
        "Credit Card Fraud": {
            "description": "Detect fraudulent credit card transactions",
            "type": "Classification",
            "domain": "Finance",
            "difficulty": "Advanced", 
            "tags": ["fraud", "detection", "finance", "imbalanced"]
        },
        "Customer Segmentation": {
            "description": "Segment customers based on purchasing behavior",
            "type": "Clustering",
            "domain": "Marketing",
            "difficulty": "Intermediate",
            "tags": ["clustering", "customers", "marketing", "segmentation"]
        }
    }
    
    # Simple matching logic
    user_text = " ".join(user_preferences.values()).lower()
    
    for name, info in all_datasets.items():
        score = 0
        for tag in info["tags"]:
            if tag in user_text:
                score += 1
        
        if score > 0:
            recommendations.append((name, info, score))
    
    # Sort by relevance score
    recommendations.sort(key=lambda x: x[2], reverse=True)
    
    return recommendations[:3]  # Return top 3

# Main App Layout
st.title("ğŸ¤– AI ML Agent - Your Personal Data Science Assistant")

# Sidebar for conversation
with st.sidebar:
    st.header("ğŸ’¬ Chat with AI Agent")
    
    # Display conversation history
    for message in st.session_state.conversation:
        if message["role"] == "user":
            st.write(f"**You:** {message['content']}")
        else:
            st.write(f"**AI:** {message['content']}")
    
    # Chat input
    user_input = st.text_input("Ask me anything about your ML project:", key="chat_input")
    
    if st.button("Send") and user_input:
        # Add user message to conversation
        st.session_state.conversation.append({"role": "user", "content": user_input})
        
        # Get AI response
        ai_response = get_ai_response(user_input)
        
        # Add AI response to conversation
        st.session_state.conversation.append({"role": "ai", "content": ai_response})
        
        # Store user preferences
        if "dataset" in user_input.lower() or "data" in user_input.lower():
            st.session_state.user_preferences["dataset_interest"] = user_input
        
        st.rerun()

# Main content area
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ¯ Your ML Project")
    
    # Show current status
    if st.session_state.user_preferences:
        st.subheader("ğŸ“‹ Project Preferences")
        for key, value in st.session_state.user_preferences.items():
            st.write(f"â€¢ **{key.replace('_', ' ').title()}:** {value}")
    
    # Dataset recommendations
    if st.session_state.user_preferences:
        st.subheader("ğŸ“Š Recommended Datasets")
        recommendations = recommend_datasets(st.session_state.user_preferences)
        
        if recommendations:
            for name, info, score in recommendations:
                with st.expander(f"ğŸ“ˆ {name} (Relevance: {score}/5)"):
                    st.write(f"**Description:** {info['description']}")
                    st.write(f"**Type:** {info['type']} | **Domain:** {info['domain']} | **Difficulty:** {info['difficulty']}")
                    
                    if st.button(f"Use {name}", key=f"use_{name}"):
                        st.session_state.current_dataset = name
                        st.success(f"Selected {name} dataset!")
                        st.rerun()
        else:
            st.info("Tell me more about your project to get dataset recommendations!")
    
    # Current dataset info
    if st.session_state.current_dataset:
        st.subheader(f"ğŸ“Š Current Dataset: {st.session_state.current_dataset}")
        st.info("Ready to start analysis! Use the sidebar to load your dataset and begin the ML pipeline.")

with col2:
    st.header("ğŸ› ï¸ Quick Actions")
    
    # Quick start buttons
    if st.button("ğŸš€ Start New Project"):
        st.session_state.conversation = []
        st.session_state.user_preferences = {}
        st.session_state.current_dataset = None
        st.rerun()
    
    if st.button("ğŸ“Š Browse Datasets"):
        st.session_state.conversation.append({
            "role": "user", 
            "content": "I want to browse available datasets"
        })
        st.rerun()
    
    if st.button("ğŸ”§ ML Pipeline Help"):
        st.session_state.conversation.append({
            "role": "user",
            "content": "Help me understand ML pipeline options"
        })
        st.rerun()
    
    if st.button("ğŸ“š Explain Results"):
        st.session_state.conversation.append({
            "role": "user", 
            "content": "I need help understanding ML results and outputs"
        })
        st.rerun()

# Footer
st.markdown("---")
st.markdown("ğŸ’¡ **Tip:** Ask me about any part of the ML process - from data selection to result interpretation!")

# Add some sample conversation starters
st.subheader("ğŸ’¬ Conversation Starters")
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("I want to predict house prices"):
        st.session_state.conversation.append({
            "role": "user",
            "content": "I want to predict house prices"
        })
        st.rerun()

with col2:
    if st.button("Help me analyze customer data"):
        st.session_state.conversation.append({
            "role": "user", 
            "content": "Help me analyze customer data"
        })
        st.rerun()

with col3:
    if st.button("Explain correlation matrix"):
        st.session_state.conversation.append({
            "role": "user",
            "content": "Explain correlation matrix"
        })
        st.rerun()
